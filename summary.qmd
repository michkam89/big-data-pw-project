# WNIOSKI I ZAKOŃCZENIE

## Budowa infrastruktury

W niniejszej pracy przygotowano podstawowe środowisko pracy do analizy zbiorów danych. Jako środowisko przechowywania danych wykorzystano serwis `S3` który pozwala na przechowywanie wiele typów oraz formatów danych i jest łatwo skalowalny oraz posiada opcje optymalizacji oraz archiwizacji danych. Do zapisania danych w serwisie `S3` wykorzystano usługę `Cloud9`, jednak równie dobrze użytkownik może wykonać opisane w tej pracy kroki na każdej maszynie (również poza chmurą AWS), wystarczy tylko wygenerowanie i użycie odpowiednich kluczy dostępu. `Cloud9` wydaje się jednak prostszym rozwiązaniem, gdyż zawiera już zainstalowany program `AWS CLI`, który może być wykorzystany do zapisania danych w serwisie S3.

Część analityczną zaprojektowano w sposób aby jak najbardziej uprościć konfigurację by móc skupić się na przeprowadzeniu analuzy danych. Wykorzystano popularne narzędzie `Apache Spark` (głównie moduł SQL) oraz serwis `AWS EMR` ze środowiskiem graficznym `Jupyter Hub`, bardzo popularnym wśród programistów/analityków w języku python. Dodatkowo zadbano o to żeby efekty pracy (notebooki) oraz logi były automatycznie zapisywane w osobnych koszykach w serwisie S3. 

W przypadku chęciu usprawnienia i optymailzacji rozwiązania możliwa jest dodatkowa konfiguracja obecnej infrastruktury. Jedym z możliwych usprawnień jest reduckja zajmowanego miesjca w serwisie `S3`. Koszyk danych `raw-data-beer-and-wine` może posiadać automatyczne usuwanie danych lub archiwizację danych w serwisie `S3 Glacier`, jako że dane w formacie `xml` po ekstrakcji i zapisaniu ich w formacie `parquet` są danymi nadmiarowymi. 

Potencjalny schemat tego rozwiązania mógłby składać się z następujących kroków:

1. Wykonywanie kodu z sekcji @sec-extraction, mogłoby zostać zautomatyzowane poprzez cykliczne uruchamianie skryptu o danej godzinie np. przy pomocy serwisu AWS Lambda bądź AWS Batch
2. Po wykonaniu ekstrakcji uruchamiany byłby kod do wstępnego przetwarzania danych zapisujący dane w formacie `parquet` opisany w @sec-prepro oraz w szczegółach w @sec-prepro-chapter
3. Dane z koszyka `raw-data-beer-and-wine` byłyby usuwane godzinę po zakończeniu kroku numer 2.

W przypadku prezentowanego w tej pracy przykładu optymalizacja przechowywania danych nie jest etapem kluczowym, ze względu na stosunkowo mały rozmiar danych, lecz będzie to miało istotne znaczenie w kosztach analizy, gdy rozmiar danych będzie zwiększony.

Kolejnym usprawnieniem w celu usprawnienienia tworzenie niezbędnej infrastruktury jest zapisanie jej w postaci projektu typu `Infrastructure as a Code`. Pozwoliłoby to na śledzenie zmian w projekcie przy pomocy kontroli wersji a także dodawanie kolejnych komponentów wspomnianych wyżej lub usuwanie komponentów w sposób programatyczny. Można w tym celu wykorzystać programy `AWS CDK`[@aws_cdk], `Terraform`[@terraform] lub bezpośrednio `AWS CloudFormation`[@aws_cf].

## Analiza danych

W części analitycznej pracy zbadano charakterystykę jednego z forów internetowych. W @sec-activity, @sec-dynamics oraz @sec-retention dokonano wstępnych analiz zachowań użytowników na forum, takich jak częstotliwość zadawania pytań, uzyskiwania odpowiedzi czy retencji na forum. Wyniki mogą posłużyć za wstęp do głębszych analiz przez właścicieli forum, w celu poszukiwania metod uatrakcyjnienia dostarczanego produktu, jako że forum z czasem znacząco traci na popularności.

W kolejnych sekcjach (@sec-qstats, @sec-tags, @sec-titles) dokonano analizy treści oraz jakości zamieszczanych na forum informacji. @sec-qstats zidentyfikowała najlepiej oraz najgorzej oceniane pytania. Ich dokłądniejsza analiza może posłużyć do zbadania treści tych wiadomości w celu stworzenia instrukcji dla nowych użytkowników o tym, co powinno znaleźć się w treści pytania, aby zwiększyć szanse na uzyskanie odpowiedzi. 
W @sec-tags oraz @sec-titles dokonano wstępnej analizy tematyki zadawanych pytań. Podejście opierało się na wyszukiwaniu najpopularniejszych słów kluczowych wśród znaczników (tagów) oraz tytułów postów. W celu pogłębienia tej analizy można spróbować wykorzystać techinkę `tf-idf` [@wikipedia_2020] w celu identyfikacji charakterystycznych słów, biorąc pod uwagę ich występowanie pomiędzy postami. Dodatkowo możliwe jest grupowanie podobnych do siebie postów przy pomocy algorytmu `TextRank` [@text_rank]. Algorytm ten można wykorzystać w celu oferowania podpowiedzi użytkownikom, którzy zadają pytanie na forum będące duplikatem już istniejącego.