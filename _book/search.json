[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Wieloskalowa analiza danych z forum internetowego przy użyciu zasobów infrastruktury chmurowej AWS oraz technologii Spark",
    "section": "",
    "text": "1 WSTĘP"
  },
  {
    "objectID": "index.html#technologie-big-data",
    "href": "index.html#technologie-big-data",
    "title": "Wieloskalowa analiza danych z forum internetowego przy użyciu zasobów infrastruktury chmurowej AWS oraz technologii Spark",
    "section": "1.1 Technologie big data",
    "text": "1.1 Technologie big data\nIlość przetwarzanych danych cyfrowych na świecie rośnie w tempie logarytmicznym i obecnie podawana jest w dziesiątkach zettabajtów (Bartley 2022). Wraz ze wzrostem ilości danych, niezbędne jest optymalizowanie bądź zwiększanie miejsca potrzebnego na ich przechowywanie oraz ulepszanie algorytmów pozwalających na dokonanie analiz w czasie umożliwiającym na szybkie wyciąganie wniosków i podejmowania stosownych akcji.\nZnaczącym usprawnieniem procesu analitycznego opartego na dużych ilościach danych było opracowanie algorytmu MapReduce (Dean i Ghhemawat 2010), wykorzystującego równoległe przetwarzanie zbiorów danych w klastrach komputerowych. Opiera się na stosowaniu dwóch “kroków”. Kroku map, odpowiadającego za wykonaywanie zadań w węzłach roboczych (niezależnie od siebie) a następnie kroku reduce, który zbiera dane z węzłów roboczych i dokonuje kroku redukcji danych poprzez np. agregację.\nKlastry komputerowe mogą być wykorzystywane nie tylko do obliczeń, ale również do przechowywania danych. Platformą, która wykorzystuje rozproszony system plików jest np. Apache Hadoop i sytem HDFS („Apache Hadoop”, b.d.). System HDFS daje możliwość na zwiększenie dostępności danych poprzez ich replikację. W przypadku niedostępności jednego z węzłów roboczych, dane są dostępne na pozostałych węzłach w postaci kopii. Dodatkowo poprzez odpowiednie partycjonowanie danych można usprawnić działania analityczne, które ograniczą wysyłanie danych po sieci klastra (Navarro 2017).\nApache Hadoop i MapReduce mają jednak swoje ograniczenia, z czego jednym z bardziej znaczących jest konieczność wczytywania danych z dysku dla każdego zadania. Jest to mało wydajne przy zadaniach wykonywanych w sposób iteracyjny jak np. trenowanie modeli statystycznych z wieloma hiperparametrami (Zaharia i in. 2010). W pracy Zaharia i in. (2010), zaprezentowano techologię Spark (obecnie Apache Spark), która obchodzi te ograniczenia poprzez wykorzystanie dużo szybszej pamięci RAM oraz wprowadzenie obiektów RDD (ang. Resilient Distributed Datasets) (Zaharia i in. 2012). RDD są obiektami niemodyfikowalnymi (ang. immutable), rozproszonymi po klastrze i co najistotniejsze przechowywane w pamięci RAM. Dzieki temu obiekty RDD mogą być wykorzystywane wielokrotnie bez konieczności wykonywania operacji odczytu z dysku, który znacząco obniża tempo wykonywania obliczeń. Autorzy Zaharia i in. (2010) szacują że Apache Spark jest około 10 razy szybszy niż Apache Hadoop w wykonywaniu iteracyjnym operacji związanych z uczeniem maszynowym."
  },
  {
    "objectID": "index.html#formaty-danych",
    "href": "index.html#formaty-danych",
    "title": "Wieloskalowa analiza danych z forum internetowego przy użyciu zasobów infrastruktury chmurowej AWS oraz technologii Spark",
    "section": "1.2 Formaty danych",
    "text": "1.2 Formaty danych\nW czasach sprzed tzw. ery Big Data, dane przechowywane były (oraz często dalej są) w relacyjnych bazach danych. W najprostszym założeniu (pomijając możliwości indeksowania oraz stosowania kluczy) dane przetrzymywane są w klasycznym schemacie, jedna obserwacja - jeden wiersz, jedna cecha - jedna kolumna (zobacz Tab. 1.1). W jaki sposób te dane są przechowywane na dysku obrazuje za to Rys. 1.1. Przechowywanie informacji w ten sposób powoduje, że aby dokonać agregacji cechy znajdującej się w kolumnie o nazwie kolumna1, algorytm musi przeskanować wszystkie wiersze wczytując je do pamięci.\nRozwiązaniem, które usprawnia ten proces i jest wykorzystywane w rozwiązaniach typu Big Data jest przechowywanie danych w sposób kolumnowy, zaprezentowany na Rys. 1.2. W tym przypadku, jeżeli naszym celem jest jedynie zagregowanie cechy kolumna1, to tylko dane z tej kolumny zostaną wczytanie do pamięci, ponieważ algorytm wie, w kórym miejscu na dysku znajdują się dane z tej kolumny. Przykładem formatów kolumnowych w systemie Apache Hadoop są RCFile oraz ORC a dodatkowo bardzo popularnym formatem jest format Apache Parquet.\nDodatkową zaletą formatów kolumnowych jest kompresja danych i zwiększenie szybkości wykonywania zapytań Tab. 1.2.\n\n\nTab. 1.1: Zobrazowanie tabeli rozrysowanej na Rys. 1.1 oraz Rys. 1.2\n\n\n\nkolumna1\nkolumna2\nkolumna3\n\n\n\n\nwiersz1\nA\nB\nC\n\n\nwiersz2\nD\nE\nF\n\n\nwiersz3\nG\nH\nI\n\n\nwiersz4\nJ\nK\nL\n\n\n\n\n\n\n\n\n\ngraph TB\n    subgraph wiersz4\n        direction LR\n        J\n        K\n        L\n    end\n    subgraph wiersz3\n        direction LR\n        G\n        H\n        I\n    end\n    subgraph wiersz2\n        direction LR\n        D\n        E\n        F\n    end\n    subgraph wiersz1 \n        direction LR\n        A\n        B\n        C\n    end\n\n\n\n\n\nRys. 1.1: Przykład wierszowego formatu danych. Kolejność wierszy przedstawia pozycję na dysku.\n\n\n\n\n\n\n\n\n\ngraph TB\n    subgraph split2\n    direction LR\n        subgraph col4[\"kolumna1\"]\n            direction LR\n            G\n            J\n        end\n        subgraph col5[\"kolumna2\"]\n            direction LR\n            H\n            K\n        end\n        subgraph col6[\"kolumna3\"]\n            direction LR\n            I\n            L\n        end\n    end\n    subgraph split1\n        subgraph kolumna1 \n            direction LR\n            A\n            D\n        end    \n        subgraph kolumna2\n            direction LR\n            B\n            E\n        end\n        subgraph kolumna3\n            direction LR\n            C\n            F\n        end\n    end\n\n\n\n\n\nRys. 1.2: Przykład kolumnowego formatu danych. Kolejność wierszy przedstawia pozycję na dysku.\n\n\n\n\n\n\nTab. 1.2: Porównanie przykładowych danych przetrzymywanych w formacie CSV oraz Apache Parquet 1\n\n\n\n\n\n\n\n\n\nFormat\nRozmiar danych\nCzas wykonania zapytania (s)\nIlość wczytanych danch\nKoszt ($)2\n\n\n\n\nCSV\n1 TB\n236\n1.15 TB\n5.75\n\n\nApache Parquet\n130 GB\n6.78\n2.51 GB\n0.01"
  },
  {
    "objectID": "index.html#wykorzystanie-zasobów-chmurowych",
    "href": "index.html#wykorzystanie-zasobów-chmurowych",
    "title": "Wieloskalowa analiza danych z forum internetowego przy użyciu zasobów infrastruktury chmurowej AWS oraz technologii Spark",
    "section": "1.3 Wykorzystanie zasobów chmurowych",
    "text": "1.3 Wykorzystanie zasobów chmurowych\nW obecnych czasach, zapotrzebowanie na możliwość szybkiej analizy dużej ilości danych staje się wartością kluczową w zyskiwaniu przewagi biznesowej nad konkurencją („Why and how companies need to take advantage of (BIG) data science” 2021). Modele biznesowe opierające się na własnej infrastukturze bądź własnym centrum analizy danych, wymagają ciągłego wsparcia i utrzymywania, niezależnie czy w danym momencie te zasoby są wykorzystywane czy też nie. Dodatkowo, w celu zwiększenia mocy analitycznych, często niezbędny jest zakup nowych maszyn, procesorów czy dysków, co znacząco wydłuża czas do otrzymywania wyników.\nRozwiązaniemm na te problemy może wyć wykorzystanie zasobów chmurowych. Jednym z wiodących dostarczycieli takich usług są Amazon Web Services (AWS), Google Cloud Platform (GCP) czy też Microsoft Azure, ale rynek ciągle rośnie i na popularności zyskują kolejni dostawcy (Maguire 2023). Dzięki wykorzystywaniu zasobów chmurowych użytkownicy są w stanie dynamicznie zwiększać lub zmniejszać ilość zasobów, z których korzystają. Mogą to robić w sposób manualny, lub w momencie gdy zapotrzebowanie wzrasta, skorzystać z usługi automatycznego skalowania. Użycie takiego podejścia ma uzasadnione zastosowanie w momencie gdy pewne analizy wykonywane są w większych odstępach czasu, np. co miesiąc (np. na potrzeby miesięcznych podsumowań) lub w przypadku sklepów internetowych, gdy ruch na stronach okresowo wzrasta kilkukrotnie w stosunku do przeciętnego obciążenia sieci (np. w okresach świąt Bożego Narodzenia czy tzw. Czarnego Piątku - ang. Black Friday).\nW rozwiązaniach chmurowych użytkownik ma do wyboru czy tworzy własną infrastrukturę i odpowiada za jej utrzymywanie oraz odpowiednie aktualizacje (imituje w ten sposób posiadanie własych zasobów, np. usługa AWS EC2 („Amazon EC2: Secure and resizable compute capacity for virtually any workload”, b.d.)) czy też korzysta z usług typu serverless, gdzie korzysta z gotowej usługi i płaci jedynie za czas jej pracy np. usługi AWS Lambda („AWS Lambda: Run code without thinking about servers or clusters”, b.d.) czy AWS Rekognition („Amazon Rekognition: Automate your image and video analysis with machine learning”, b.d.). Korzystanie z usług serverless odbywa się jednak kosztem braku możliwości wpływu na sposób działania aplikacji, którą w części przypadków musi potraktować jako rozwiązanie typu black-box. Przykładem może być wspomniana usługa AWS Rekognition, która jest usługą z dziedziny uczenia maszynowego do analizy plików wideo oraz audio. Użytkownik nie wie jakie modele statystyczne odpowiadają za analizę danych oraz wynik końcowy."
  },
  {
    "objectID": "index.html#cel-pracy",
    "href": "index.html#cel-pracy",
    "title": "Wieloskalowa analiza danych z forum internetowego przy użyciu zasobów infrastruktury chmurowej AWS oraz technologii Spark",
    "section": "1.4 Cel pracy",
    "text": "1.4 Cel pracy\nCelem niniejszej pracy jest utworzenie infrastruktury w chmurze obliczeniowej AWS pozwalające na wielkoskalową analizy danych w sytemie rozproszonym przy użyciu technologii Spark.\nDo stworzenia przykładowego projektu wykorzystano dane ze strony Stack Exchange („Stack Exchange”, b.d.) zawierającej zestawy danych pochodzące z forów społecznościoowych. Analizę ograniczono do danych pochodzących z forum o nazwie Beer, Wine and Spirits („Stack Exchange - Beer, Wine and Spirits”, b.d.)\nWyniki niniejszej pracy mogą posłużyć jako podstawa dla innych projektów, chcących rozszerzać wiedzę o zachowaniach użytkowników oraz poruszanej tematyki na różnych forach internetowych.\n\n\n\n\n„Amazon EC2: Secure and resizable compute capacity for virtually any workload”. b.d. Amazon Web Services, Inc. https://aws.amazon.com/ec2/.\n\n\n„Amazon Rekognition: Automate your image and video analysis with machine learning”. b.d. Amazon Web Services, Inc. https://aws.amazon.com/rekognition/.\n\n\n„Apache Hadoop”. b.d. https://hadoop.apache.org/.\n\n\n„AWS Lambda: Run code without thinking about servers or clusters”. b.d. Amazon Web Services, Inc. https://aws.amazon.com/lambda/.\n\n\nBartley, Kevin. 2022. „Data Statistics - how much data is there in the world?” Rivery, listopad. https://rivery.io/blog/big-data-statistics-how-much-data-is-there-in-the-world/.\n\n\nDean, Jeffrey, i Sanjay Ghhemawat. 2010. System and method for efficient large-scale data processing, issued styczeń 2010.\n\n\nMaguire, James. 2023. „Top 16 cloud service providers and companies in 2023”. Datamation. https://www.datamation.com/cloud/cloud-service-providers/.\n\n\nNavarro, Álvaro. 2017. „Understanding the data partitioning technique”. Datio. https://www.datio.com/iaas/understanding-the-data-partitioning-technique/.\n\n\n„Stack Exchange”. b.d. Hot Questions - Stack Exchange. https://stackexchange.com/.\n\n\n„Stack Exchange - Beer, Wine and Spirits”. b.d. Stack Exchange Data Explorer. https://data.stackexchange.com/beer/queries.\n\n\n„Why and how companies need to take advantage of (BIG) data science”. 2021. Exa Futures. https://exafutures.com/en/why-how-companies-need-to-take-advantage-of-big-data-science.\n\n\nZaharia, Matei, Mosharaf Chowdhury, Tathagata Das, Ankur Dave, Justin Ma, Murphy McCauley, Michael J. Franklin, Scott Shenker, i Ion Stoica. 2012. „Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing”. http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf.\n\n\nZaharia, Matei, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, i Ion Stoica. 2010. „Spark: Cluster Computing with Working Sets”. http://people.csail.mit.edu/matei/papers/2010/hotcloud_spark.pdf."
  },
  {
    "objectID": "00_schema.html#sec-extraction",
    "href": "00_schema.html#sec-extraction",
    "title": "1  Schemat infrastruktury rozwiązania",
    "section": "1.1 Ekstrakcja danych surowych",
    "text": "1.1 Ekstrakcja danych surowych\nDo etapu ekstrakcji danych wykorzystano usługę Cloud9, która zapewnia dostęp do terminala maszyny wirtualnej z systemem linux (platforma Amazon Linux 2, typ instancji t2.micro). Z użyciem tej usługi dane zostały pobrane ze źródła w binarnym formacie 7z a następnie pliki zostały wyekstrahowane w formacie xml przy pomocy programu p7zip. Dane w formacie xml zostały następnie skopiowane do serwisu S3, gdzie utworzono koszyk danych (ang. bucket) o nazwie raw-data-beer-and-wine, którego przeznaczeniem jest przetwymywanie danych nieprzetworzonych.\nPowyższe operacje zostały wykonane przy użyciu poniższych poleceń:\n# instalacja programu p7zip\nsudo yum install p7zip.x86_64\n\n# pobranie danych\nwget https://archive.org/download/stackexchange/beer.stackexchange.com.7z\n\n# ekstrakcja danych do folderu raw-data\n7za e  beer.stackexchange.com.7z -oraw-data\n\n# zapis danych do koszyka S3 przy użyciu programu `AWS CLI`\naws s3 cp $(pwd)/raw-data s3://raw-data-beer-and-wine/ --recursive  --include \"*.xml\""
  },
  {
    "objectID": "00_schema.html#sec-prepro",
    "href": "00_schema.html#sec-prepro",
    "title": "1  Schemat infrastruktury rozwiązania",
    "section": "1.2 Przygotowanie danych wstępnych",
    "text": "1.2 Przygotowanie danych wstępnych\nW celu przygotowania danych do analizy, dane surowe zostały wstępnie przetworzone oraz zapisane w formacie parquet w koszyku danych preprocessed-data-beer-and-wine, co pozwoliło na wydajniejsze wczytywanie danych podczas uruchomień programu. Podczas etapu wstępnego przetwarzania danych, oprócz zmiany formatu plików, zdefiniowane zostały także schematy danych, które zapewnią, że kolumny wczytanych danych będą posiadały odpowiednie typy oraz, że krytyczne dane nie będą zawierały pustych wartości (Sekcja 2.2). Dodatkowo kolumny z wartościami tekstowymi, niesłownikowanymi zostały oczyszczone z tagów html oraz poddane standardowej procedurze oczyszania tekstu (Sekcja 2.3).\nPowyższe czynności zostały wykonane w notatniku typu Jupiter (ang. Jupyter Notebook) w serwisie AWS EMR. Stworzono klaster EMR (wersja 6.8.0) z instalacją Hadoop 3.2.1, Jupyter Hub 1.4.1 oraz Spark 3.3.0. Klaster składał się z 1 instancji typu master oraz 2 instancji typu core, każda typu m4.xlarge. W celu ograniczenia kosztów jako opcję zakupu wybrano typ spot z limitem maksymalnym ceny odpowiadającej typowi on-demand. Wielkość dysków EBS stworzonych instancji wynosiła 32 GiB dla każdej istancji w klastrze.\nDostęp do Jupyter Hub w utworzonym klastrze jest możliwy poprzez połącznie przez przeglądarkę z środowiskiem graficznym Jupyter Hub wykorzystując publiczny adres DNS instancji master i port 9443. Dodatkowo w celu ułatwienia dostępu do Jupyter Notebook’ów, utworzony klaster został skonfigurowany na automatycznie ich zapisywanie w dedykowanym koszyku danych w serwisie S3 (patrz Sekcja A.3)\nPolecenia programu AWS CLI odpowiadające za utworzenie klastra, wraz z niezbędnymi dodatkowymi plikami zajdują się w Załącznik A. Wszystkie serwisy AWS na potrzeby tego projektu zostały utworzone w sposób programatyczny przy użyciu programu AWS CLI (poza Cloud9, który został utworzony z poziomu konsoli zarządzającej)."
  },
  {
    "objectID": "01_preprocessing.html#konfiguracja-aplikacji-spark",
    "href": "01_preprocessing.html#konfiguracja-aplikacji-spark",
    "title": "2  Wstępna obróbka danych",
    "section": "2.1 Konfiguracja aplikacji Spark",
    "text": "2.1 Konfiguracja aplikacji Spark\nW celu przygotowania danych do analizy zostały one wstępnie przetworzone. Pierwszym etapem wstępnego przetwarzania jest wczytanie danych do środowiska analitycznego. Dane surowe, przechowywane w koszyku raw-data-beer-and-wine znajowały się w mało przyjaznym dla analiz formacie xml. Wczytanie tego typu danych wymagało załadowania dodatkowego pakietu jar o nazwie spark-xml_2.12:0.15.0 pobranego z repozytorium maven.\nW serwisie EMR można dodać tego typu pakiety wykorzystując specjalne polecenia typy Sparkmagic rozpoczynające się od znaków %%. W tym przypadku użyto %%configure:\n\n\nKod\n%%configure -f\n{\n    \"conf\": {\n        \"spark.jars.packages\": \"com.databricks:spark-xml_2.12:0.15.0\"\n    }\n}"
  },
  {
    "objectID": "01_preprocessing.html#sec-schemas",
    "href": "01_preprocessing.html#sec-schemas",
    "title": "2  Wstępna obróbka danych",
    "section": "2.2 Schematy danych",
    "text": "2.2 Schematy danych\nW celu zapewnienia wczytania danych o oczekiwanych typach oraz zapewnieniu że nie ma tam danych brakujących utworzono schematy danych, wykorzystywane w kroku wczytywania z plików xml. Poniżej przedstawiono schematy dla każdego z przetwarzanych plików oraz przykładowe wiersze.\n\n2.2.1 Plik Users\n\n\nKod\nusers_schema = StructType([\n    StructField('_AboutMe', StringType(), True),\n    StructField('_AccountId', IntegerType(), True),\n    StructField('_CreationDate', TimestampType(), True),\n    StructField(\"_DisplayName\", StringType(), True),\n    StructField(\"_DownVotes\", IntegerType(), True),\n    StructField(\"_Id\", IntegerType(), True),\n    StructField(\"_LastAccessDate\", TimestampType()),\n    StructField(\"_Location\", StringType(), True),\n    StructField(\"_ProfileImageUrl\", StringType(), True),\n    StructField(\"_Reputation\", IntegerType(), True),\n    StructField(\"_UpVotes\", IntegerType(), True),\n    StructField(\"_Views\", IntegerType(), True),\n    StructField(\"_WebsiteUrl\", StringType(), True)\n])\n\n\n\n\n-RECORD 0------------------------------------------------------------------------\n _AboutMe         | <p>Hi, I'm not really a person.</p>\\n\\n<p>I'm a backgroun... \n _AccountId       | -1                                                           \n _CreationDate    | 2014-01-21 17:45:53.587                                      \n _DisplayName     | Community                                                    \n _DownVotes       | 478                                                          \n _Id              | -1                                                           \n _LastAccessDate  | 2014-01-21 17:45:53.587                                      \n _Location        | on the server farm                                           \n _ProfileImageUrl | null                                                         \n _Reputation      | 1                                                            \n _UpVotes         | 2                                                            \n _Views           | 5                                                            \n _WebsiteUrl      | http://meta.stackexchange.com/                               \nonly showing top 1 row\n\n\n\n\n\n2.2.2 Plik Tags\n\n\nKod\ntags_schema = StructType([\n    StructField('_Count', IntegerType(), True),\n    StructField('_ExcerptPostId', IntegerType(), True),\n    StructField('_Id', IntegerType(), True),\n    StructField(\"_TagName\", StringType(), True),\n    StructField(\"_WikiPostId\", IntegerType(), True)\n])\n\n\n\n\n+------+--------------+---+-----------+-----------+\n|_Count|_ExcerptPostId|_Id|   _TagName|_WikiPostId|\n+------+--------------+---+-----------+-----------+\n|    17|          5062|  1|       hops|       5061|\n|    85|          7872|  2|    history|       7871|\n|    69|          4880|  4|    brewing|       4879|\n|    37|          5109|  5|    serving|       5108|\n|    31|           304|  6|temperature|        303|\n+------+--------------+---+-----------+-----------+\nonly showing top 5 rows\n\n\n\n\n\n2.2.3 Plik Votes\n\n\nKod\nvotes_schema = StructType([\n    StructField('_BountyAmount', IntegerType(), True),\n    StructField('_CreationDate', TimestampType(), True),\n    StructField('_Id', IntegerType(), True),\n    StructField(\"_PostId\", StringType(), True),\n    StructField(\"_UserId\", IntegerType(), True),\n    StructField(\"_VoteTypeId\", IntegerType(), True)\n])\n\n\n\n\n+-------------+-------------------+---+-------+-------+-----------+\n|_BountyAmount|      _CreationDate|_Id|_PostId|_UserId|_VoteTypeId|\n+-------------+-------------------+---+-------+-------+-----------+\n|         null|2014-01-21 00:00:00|  1|      1|   null|          2|\n|         null|2014-01-21 00:00:00|  2|      1|   null|          2|\n|         null|2014-01-21 00:00:00|  3|      4|   null|          2|\n|         null|2014-01-21 00:00:00|  4|      1|   null|          2|\n|         null|2014-01-21 00:00:00|  5|      4|   null|          2|\n+-------------+-------------------+---+-------+-------+-----------+\nonly showing top 5 rows\n\n\n\n\n\n2.2.4 Plik Posts\n\n\nKod\nposts_schema = StructType([\n    StructField('_AcceptedAnswerId', IntegerType(), True),\n    StructField('_AnswerCount', IntegerType(), True),\n    StructField('_Body', StringType(), True),\n    StructField(\"_ClosedDate\", TimestampType(), True),\n    StructField(\"_CommentCount\", IntegerType(), True),\n    StructField(\"_CommunityOwnedDate\", TimestampType(), True),\n    StructField(\"_ContentLicense\", StringType(), True),\n    StructField(\"_CreationDate\", TimestampType(), True),\n    StructField(\"_FavoriteCount\", IntegerType(), True),\n    StructField(\"_Id\", IntegerType(), True),\n    StructField(\"_LastActivityDate\", TimestampType(), True),\n    StructField(\"_LastEditDate\", TimestampType(), True),\n    StructField(\"_LastEditorDisplayName\", StringType(), True),\n    StructField(\"_LastEditorUserId\", IntegerType(), True),\n    StructField(\"_OwnerDisplayName\", StringType(), True),\n    StructField(\"_OwnerUserId\", IntegerType(), True),\n    StructField(\"_ParentId\", IntegerType(), True),\n    StructField(\"_PostTypeId\", IntegerType(), True),\n    StructField(\"_Score\", IntegerType(), True),\n    StructField(\"_Tags\", StringType(), True),\n    StructField(\"_Title\", StringType(), True),\n    StructField(\"_ViewCount\", IntegerType(), True),\n])\n\n\n\n\n-RECORD 0------------------------------------------------------------------------------\n _AcceptedAnswerId      | 4                                                            \n _AnswerCount           | 1                                                            \n _Body                  | <p>I was offered a beer the other day that was reportedly... \n _ClosedDate            | null                                                         \n _CommentCount          | 0                                                            \n _CommunityOwnedDate    | null                                                         \n _ContentLicense        | CC BY-SA 3.0                                                 \n _CreationDate          | 2014-01-21 20:26:05.383                                      \n _FavoriteCount         | null                                                         \n _Id                    | 1                                                            \n _LastActivityDate      | 2014-01-21 22:04:34.977                                      \n _LastEditDate          | 2014-01-21 22:04:34.977                                      \n _LastEditorDisplayName | null                                                         \n _LastEditorUserId      | 8                                                            \n _OwnerDisplayName      | null                                                         \n _OwnerUserId           | 7                                                            \n _ParentId              | null                                                         \n _PostTypeId            | 1                                                            \n _Score                 | 21                                                           \n _Tags                  | <hops>                                                       \n _Title                 | What is a citra hop, and how does it differ from other hops? \n _ViewCount             | 2434                                                         \nonly showing top 1 row\n\n\n\n\n\n2.2.5 Plik Post Links\n\n\nKod\nlinks_schema = StructType([\n    StructField(\"_CreationDate\", TimestampType()),\n    StructField(\"_Id\", IntegerType()),\n    StructField(\"_LinkTypeId\", IntegerType()),\n    StructField(\"_PostId\", IntegerType()),\n    StructField(\"_RelatedPostId\", IntegerType())\n])\n\n\n\n\n+-----------------------+---+-----------+-------+--------------+\n|_CreationDate          |_Id|_LinkTypeId|_PostId|_RelatedPostId|\n+-----------------------+---+-----------+-------+--------------+\n|2014-01-21 21:04:25.23 |25 |3          |29     |25            |\n|2014-01-21 21:42:09.103|89 |1          |83     |50            |\n|2014-01-21 21:50:41.313|95 |1          |86     |2             |\n|2014-01-21 22:07:35.783|101|3          |47     |99            |\n|2014-01-21 22:13:51.38 |102|1          |74     |3             |\n+-----------------------+---+-----------+-------+--------------+\nonly showing top 5 rows\n\n\n\n\n\n2.2.6 Plik Post History\n\n\nKod\nhistory_schema = StructType([\n    StructField(\"_Comment\", StringType()),\n    StructField(\"_ContentLicense\", StringType()),\n    StructField(\"_CreationDate\", TimestampType()),\n    StructField(\"_Id\", IntegerType()),\n    StructField(\"_PostHistoryTypeId\", IntegerType()),\n    StructField(\"_PostId\", IntegerType()),\n    StructField(\"_RevisionGUID\", StringType()),\n    StructField(\"_Text\", StringType()),\n    StructField(\"_UserDisplayName\", StringType()),\n    StructField(\"_UserId\", IntegerType()),\n])\n\n\n\n\n-RECORD 0--------------------------------------------------------------------------\n _Comment           | null                                                         \n _ContentLicense    | CC BY-SA 3.0                                                 \n _CreationDate      | 2014-01-21 20:26:05.383                                      \n _Id                | 1                                                            \n _PostHistoryTypeId | 2                                                            \n _PostId            | 1                                                            \n _RevisionGUID      | a17002a0-00b0-417b-a404-0d8864bbbca5                         \n _Text              | I was offered a beer the other day that was reportedly ma... \n _UserDisplayName   | null                                                         \n _UserId            | 7                                                            \nonly showing top 1 row\n\n\n\n\n\n2.2.7 Plik Badges\n\n\nKod\nbadges_schema = StructType([\n    StructField(\"_Class\", IntegerType()),\n    StructField(\"_Date\", TimestampType()),\n    StructField(\"_Id\", IntegerType()),\n    StructField(\"_Name\", StringType()),\n    StructField(\"_TagBased\", BooleanType()),\n    StructField(\"_UserId\", IntegerType()),\n])\n\n\n\n\n+------+----------------------+---+--------------+---------+-------+\n|_Class|_Date                 |_Id|_Name         |_TagBased|_UserId|\n+------+----------------------+---+--------------+---------+-------+\n|3     |2014-01-21 20:52:16.97|1  |Autobiographer|false    |1      |\n|3     |2014-01-21 20:52:16.97|2  |Autobiographer|false    |2      |\n|3     |2014-01-21 20:52:16.97|3  |Autobiographer|false    |6      |\n|3     |2014-01-21 20:52:16.97|4  |Autobiographer|false    |7      |\n|3     |2014-01-21 20:52:16.97|5  |Autobiographer|false    |9      |\n+------+----------------------+---+--------------+---------+-------+\nonly showing top 5 rows"
  },
  {
    "objectID": "01_preprocessing.html#sec-text-cleanup",
    "href": "01_preprocessing.html#sec-text-cleanup",
    "title": "2  Wstępna obróbka danych",
    "section": "2.3 Czyszczenie kolumn tekstowych",
    "text": "2.3 Czyszczenie kolumn tekstowych\nPliki Users, History oraz Posts zawierają koumny tekstowe z danymi wpisywanymi przez użytkowników oraz często zawierające znaki specjalne czy tagi html. W związku z tym kolumny _AboutMe (plik Users), _Text (plik History) oraz _Body (plik Posts) zostały poddane procesowi oczyszczania.\nW tym celu utworzono funkcję UDF o nazwie tags_remove() Sekcja B.1, która odpowiada za usunięcie tagów html.\nDodatkowo znaki \\n, \\t, \\r oraz podwójne spacje zastąpiono pojedynczymi znakami spacji a także usunięto znaki spacji z początków i końców wartości tekstowych.\nPoniżej zaprezentowano przykłady kolumn nieoczyszczonych oraz po ich oczyszczeniu (zawierające końcówkę _clean):\n\nDla pliku Users: \n\n\n\n-RECORD 0----------------------------------------------------------------------\n _AboutMe       | <p>Hi, I'm not really a person.</p>\\n\\n<p>I'm a backgroun... \n _AboutMe_clean | Hi, I'm not really a person. I'm a background process tha... \n-RECORD 1----------------------------------------------------------------------\n _AboutMe       | <p>Dev #2 who helped create Stack Overflow currently work... \n _AboutMe_clean | Dev #2 who helped create Stack Overflow currently working... \n-RECORD 2----------------------------------------------------------------------\n _AboutMe       | <p>Former Stack Exchange employee</p>\\n                      \n _AboutMe_clean | Former Stack Exchange employee                               \n-RECORD 3----------------------------------------------------------------------\n _AboutMe       | \\n<p>Developer at Stack Overflow focusing on public Q&amp... \n _AboutMe_clean | Developer at Stack Overflow focusing on public Q&A. Russi... \n-RECORD 4----------------------------------------------------------------------\n _AboutMe       | <p><strong>BY DAY:</strong> I execute projects on both mo... \n _AboutMe_clean | BY DAY: I execute projects on both mobile and on the web ... \nonly showing top 5 rows\n\n\n\n[Stage 7:>                                                          (0 + 1) / 1]                                                                                \n\n\n\nDla pliku History: \n\n\n\n-RECORD 0-------------------------------------------------------------------\n _Text       | I was offered a beer the other day that was reportedly ma... \n _Text_clean | I was offered a beer the other day that was reportedly ma... \n-RECORD 1-------------------------------------------------------------------\n _Text       | What is a citra hop, and how does it differ from other hops? \n _Text_clean | What is a citra hop, and how does it differ from other hops? \n-RECORD 2-------------------------------------------------------------------\n _Text       | <hops>                                                       \n _Text_clean |                                                              \n-RECORD 3-------------------------------------------------------------------\n _Text       | As far as we know, when did humans first brew beer, and w... \n _Text_clean | As far as we know, when did humans first brew beer, and w... \n-RECORD 4-------------------------------------------------------------------\n _Text       | When was the first beer ever brewed?                         \n _Text_clean | When was the first beer ever brewed?                         \nonly showing top 5 rows\n\n\n\n/config/workspace/env/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n  warnings.warn(\n\n\n\nDla pliku Posts: \n\n\n\n-RECORD 0-------------------------------------------------------------------\n _Body       | <p>I was offered a beer the other day that was reportedly... \n _Body_clean | I was offered a beer the other day that was reportedly ma... \n-RECORD 1-------------------------------------------------------------------\n _Body       | <p>As far as we know, when did humans first brew beer, an... \n _Body_clean | As far as we know, when did humans first brew beer, and w... \n-RECORD 2-------------------------------------------------------------------\n _Body       | <p>How is low/no alcohol beer made? I'm assuming that the... \n _Body_clean | How is low/no alcohol beer made? I'm assuming that the be... \n-RECORD 3-------------------------------------------------------------------\n _Body       | <p>Citra is a registered trademark since 2007. Citra Bran... \n _Body_clean | Citra is a registered trademark since 2007. Citra Brand h... \n-RECORD 4-------------------------------------------------------------------\n _Body       | <p>In general, what's the best way to work out the temper... \n _Body_clean | In general, what's the best way to work out the temperatu... \nonly showing top 5 rows\n\n\n\nTak przygotowane dane, w celu optymalizacji miejsca zajmowanego w koszyku S3, oraz w celu przyspieszenia procesu wczytywania danych zapisano w formacie parquet. Zaskutkowało to redukcją zajmowanego miejsca w koszuku o około 50%."
  },
  {
    "objectID": "02_analytics.html#sec-activity",
    "href": "02_analytics.html#sec-activity",
    "title": "3  Analiza danych",
    "section": "3.1 Analiza aktywności użytkowników na forum",
    "text": "3.1 Analiza aktywności użytkowników na forum\nJako pierwsze postanowiono zbadać czy forum jest aktywne. W tym celu liczbę postów zagregowano w miesięczne insterwały i zliczono ich ilość. Pierwsza wiadomość pojawiła się na forum 2014-01-21 natomiast ostatnia 2022-06-05. Na przestrzeni tych ~8,5 roku pojawiło się 3769 postów. Z Rys. 3.1 widać, że zainteresowanie forum spadało w czasie. W rekordowych pierwszych 2 miesiącach umieszczano ich odpowiednio 413 oraz 190, natomiast pod koniec badanego okresu wartości często nie przekraczały 10.\n\n\n\n\n\nRys. 3.1: Liczba postow w czasie z podzialem na pytania i odpowiedzi. Lewy panel wskazuje surowe wartości liczby postów, prawy wartości logarytmiczne o podstawie 10. Zamieszczono również linię trendu obliczoną przy użyciu algorytmu LOESS (ang. Local Polynomial Regression Fitting)\n\n\n\n\nStosunek ilości odpowiedzi do zadawanych pytań (rato) rósł do roku 2021, w którym to zaczęto odnotowywać spadek. Na wzrost ratio miała wpływ zmniejszająca się ilość zadawanych pytań (maleje próba badana) co zostało przedstawione na Rys. 3.2. Patrząc na wartości średnie tylko co drugie pytanie uzyskiwało odpowiedź (ratio 0.48 +/- 0.22). Ogólne statystyki stosunku pytań do dopowiedzi w czasie zostały przedstawione w Tab. 3.1.\n\n\n\n\n\nRys. 3.2: Stosunek ilości odpowiedzi na zadane pytania w czasie, uwzględniając ilość zadanych pytań\n\n\n\n\n\n\n\n\n\n\nTab. 3.1:  Statystyki stosunku ilości odpowiedzi na pytania. Bez uwzględnienia czy było ono zaakceptowane czy też nie. \n  \n    \n      \n      średnia\n      odchylenie standardowe\n      min\n      max\n      mediana\n    \n  \n  \n    \n      0\n      0.476316\n      0.219824\n      0.0\n      1.5\n      0.444444"
  },
  {
    "objectID": "02_analytics.html#sec-dynamics",
    "href": "02_analytics.html#sec-dynamics",
    "title": "3  Analiza danych",
    "section": "3.2 Dynamika oraz statystyki udzielanych odpowiedzi",
    "text": "3.2 Dynamika oraz statystyki udzielanych odpowiedzi\nTwórcy pytań na forum mają możliwość wybrania odpowiedzi, która jest najtrafniejsza i zawiera poprawną odpowiedź. Te odpowiedzi nazywane są zaakceptowanymi odpowiedziami. Zbadano jak często najwyżej oceniona odpowiedź nie była zaakceptowaną odpowiedzią.\nOkazało się, że w przypadku pytań, które posiadały jakąkolwiek odpowiedź, około 12.7% (646) przypadków najlepiej ocenianych odpowiedzi nie było tą, która została zaakceptowana przez autora.\nNastępnie porównano oceny odpowiedzi zaakceptowanych z pozostałymi odpowiedziami a statystyki przedstawiono w Tab. 3.2 oraz Rys. 3.3.\nPośród odpowiedzi na zadawane pytania wyróżniono 3 kategorie:\n\nodpowiedź zaakceptowana (is_accepted=True)\nodpowiedź niezaakceptowana (is_accepted=False)\nodpowiedź niezaakceptowana ale równocześnie brak jest zaakceptowanej odpowiedzi na to pytanie (is_accepted=None)\n\nZ analizy wynika, iż zaakceptowane odpowiedzi mają średnio wyższe oceny użytkowników (6,39) niż pozostałe oceny (2.75 - is_accepted=None; 2.58 - is_accepted=False), co było oczekiwanym wynikiem.\nNastępnie zbadano jak szybko od pojawienia się pytania, pojawia się zaakceptowana odpowiedź. Dla wszystkich pytań na tym forum jest to średnio 25244 minuty, ale wartość środkowa (753) sugeruje, że średnia może być zaburzona. W celu obliczenia średniej nie zaburzonej tak dużymi wartościami odstającymi odrzucono wartości znajdujące powyżej 6 odchyleń standardowych od średniej (75894 minut). Tym razem uzyskano wartość 3988 minut (~66,5 godziny), przy wartości środkowej 628 (~10,5 godziny).\nRozkłady wartości przedstawiono na Rys. 3.4 oraz w Tab. 3.3 dla całego zbioru danych oraz Tab. 3.4 po odrzuceniu wartości odstających.\n\n\n\n\n\n\nTab. 3.2:  Statystyki ocen odpowiedzi zaakceptowanych w stosunku do pozostałych \n  \n    \n      \n      is_accepted\n      średnia\n      odchylenie standardowe\n      min\n      max\n      mediana\n    \n  \n  \n    \n      0\n      None\n      2.755169\n      3.181837\n      -5\n      30\n      2\n    \n    \n      1\n      True\n      6.395044\n      5.915949\n      0\n      46\n      5\n    \n    \n      2\n      False\n      2.584169\n      2.735329\n      -4\n      26\n      2\n    \n  \n\n\n\n\n\n\n\n\n\n\nRys. 3.3: Rozkład ocen pytań zaakceptowanych w porównaniu do pozostałych\n\n\n\n\n\n\n\n\n\nRys. 3.4: Rozkład czasu od pojawienia się pytania do zaakceptowanej odpowiedzi w minutach\n\n\n\n\n\n\n\n\n\n\nTab. 3.3:  Statystyki czasu od pojawienia się pytania do zaakceptowanej odpowiedzi w minutach \n  \n    \n      \n      średnia\n      odchylenie standardowe\n      min\n      max\n      mediana\n    \n  \n  \n    \n      0\n      25244.943571\n      123338.63251\n      0.0\n      1506239.05\n      753.25\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nTab. 3.4:  Statystyki ocen odpowiedzi zaakceptowanych w stosunku do pozostałych po odrzuceniu wartości odstających (> 6*SD) \n  \n    \n      \n      średnia\n      odchylenie standardowe\n      min\n      max\n      mediana\n    \n  \n  \n    \n      0\n      3988.821096\n      9775.983836\n      0.0\n      73465.9\n      628.72"
  },
  {
    "objectID": "02_analytics.html#sec-retention",
    "href": "02_analytics.html#sec-retention",
    "title": "3  Analiza danych",
    "section": "3.3 Retencja użytkowników",
    "text": "3.3 Retencja użytkowników\nW całej historii forum zarejestrowanych zostało 8948 użytkowników, z czego jedno konto jest kontem bota. Miarą retencji użytkownika na forum określono czas od utworzenia konta to ostaniej zamieszczonej wiadomości. Wśród 50 najdłużej aktywnych kont zidentyfikowano konto z wynikiem aż 3052 dni a konto z ostatnim indeksem w tej grupie miało wynik 2128 dni. Jednakże patrząc na całą populację, wartość środkowa wyniosła 11, a 50% wartości mieści się pomiędzy 0 a 594 dniami. Użytkownicy z wartością 0, są to najprawdopodobniej użytkowicy, którzy zadali jedyne pytanie na forum w dniu założenia konta (410 użytkowników). Czas na forum najdłużej aktywnych użytkowników został zwizualizowany na Rys. 3.5. Dodatkowo okazało się, że 7691 (86%) kont było biernymi użytkownikami forum i nidgy nie dodało żadnego posta.\n\n\n\n\n\nRys. 3.5: Czas na forum 50 najdłużej aktywnych kont"
  },
  {
    "objectID": "02_analytics.html#sec-qstats",
    "href": "02_analytics.html#sec-qstats",
    "title": "3  Analiza danych",
    "section": "3.4 Statystyki najwyżej oraz najniżej ocenianych pytań",
    "text": "3.4 Statystyki najwyżej oraz najniżej ocenianych pytań\nZbadano statystyki zadawanych pytań. Użytkownicy forum mogą oceniać pojawiające się tam pytania czego miarą jest wartość score. Sprawdzono, czy długość zadanego pytania ma wpływ na jego ocenę.\nŚrednia długość pytania wyniosła 415 znaków (+/- 330) a wartość środkowa 331 znaków. Najdłuższe pytanie posiadało 3133 znaki a najkrótsze jedynie 30. Statystyki przedstawiono w Tab. 3.5.\n\n\n\n\n\n\nTab. 3.5:  Statystyki długości postów (liczba znaków) \n  \n    \n      \n      średnia\n      odchylenie standardowe\n      max\n      min\n      mediana\n    \n  \n  \n    \n      0\n      415.863676\n      330.836043\n      3133\n      30\n      331\n    \n  \n\n\n\n\n\nŚrednio pytanie było oceniane na wartość 6.28 (+/-5.88) a wartość środkowa wyniosła 5. Najlepiej oceniane pytanie miało ocenę 67 a najgorzej -7. Statystyki zestawiono w Tab. 3.6.\n\n\n\n\n\n\nTab. 3.6:  Statystyki ocen użytkowników \n  \n    \n      \n      średnia\n      odchylenie standardowe\n      max\n      min\n      mediana\n    \n  \n  \n    \n      0\n      6.278804\n      5.876114\n      68\n      -7\n      5\n    \n  \n\n\n\n\n\nZależność pomiędzy wartością score a długością wiadomości przedstawiono na Rys. 3.6. Ze względu na obecność dużej ilości wartości odstających obie wartości przedstawiono również w skali logarymicznej.\nNie wykryto korelacji pomiędzy tymi dwoma zmiennymi. Współczynnik korelacji wyniósł -0.048 oraz -0.018 dla wartości zlogarytmowanych.\nPostanowiono dodatkowo zbadać dwie podgrupy danych dla pytań najlepiej oraz najgorzej ocenianych. Wyodrębniono po 100 pytań z każdej z grup. Wyniki dla tych grup przestwationo na Rys. 3.6 (dolny panel) oraz w Tab. 3.7 i Tab. 3.8.\nW podgrupie najlepiej ocenianych pytań średnia ocena wyniosła 20.6 +/- 8.5 a w podgrupie najgorzej ocenianych 0.4 +/- 1.2. Rozdzielność tych statystyk dla tych grup była oczekiwanym wynikiem.\nStatystyki długości pytań nie odbiegają od siebie w znaczący sposób (średnie 349 +/- 275 najlepiej oceniane oraz 389 +/- 424 najgorzej oceniane) sugerując iż to długość pytania nie ma znaczącego wpływu na jego ocenę.\n\n\n\n\n\nRys. 3.6: Oceny pytań (score) na forum w zależności od ilości znaków zawartych w pytaniu\n\n\n\n\n\n\n\n\n\n\nTab. 3.7:  Statystyki długości postów z podziałem na podgrupy najlepszych (type=top) i najgorszych pytań (type=bottom) \n  \n    \n      \n      type\n      średnia\n      odchylenie standardowe\n      max\n      min\n      mediana\n    \n  \n  \n    \n      0\n      top\n      349.08\n      275.088587\n      1823\n      46\n      261\n    \n    \n      1\n      bottom\n      389.26\n      424.654836\n      3133\n      30\n      270\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nTab. 3.8:  Statystyki ocen użytkowników z podziałem na podgrupy najlepszych (type=top) i najgorszych pytań (type=bottom) \n  \n    \n      \n      type\n      średnia\n      odchylenie standardowe\n      max\n      min\n      mediana\n    \n  \n  \n    \n      0\n      top\n      20.6\n      8.551685\n      68\n      14\n      18\n    \n    \n      1\n      bottom\n      0.4\n      1.206045\n      1\n      -7\n      1\n    \n  \n\n\n\n\n\nZauważono również że najlepiej oceniane pytania są również częściej oglądane i mają więcej odpowiedzi niż pytania najgorzej oceniane (Rys. 3.7). Najlepiej oceniane pytania mają średnio 4 odpowiedzi podczas gdy najgorzej jedynie 1 (Tab. 3.9) oraz odpowiednio średnio 26289 i 475 wyświetleń (Tab. 3.10)\n\n\n\n\n\nRys. 3.7: Liczba odpowiedzi oraz oceny najlepiej (type=top) oraz najgorzej (type=bottom) ocenianych pytań w zależności od liczby ich wyświetleń\n\n\n\n\n\n\n\n\n\n\nTab. 3.9:  Statystyki ilości odpowiedzi na pytania z podziałem na podgrupy najlepszych (type=top) i najgorszych (type=bottom) pytań \n  \n    \n      \n      type\n      średnia\n      odchylenie standardowe\n      max\n      min\n      mediana\n    \n  \n  \n    \n      0\n      top\n      3.96\n      2.581676\n      17\n      1\n      3\n    \n    \n      1\n      bottom\n      1.28\n      0.964836\n      4\n      0\n      1\n    \n  \n\n\n\n\n\n\n\n\n\n\n\nTab. 3.10:  Statystyki ilości wyświetleń pytań z podziałem na podgrupy najlepszych (type=top) i najgorszych (type=bottom) pytań \n  \n    \n      \n      type\n      średnia\n      odchylenie standardowe\n      max\n      min\n      mediana\n    \n  \n  \n    \n      0\n      top\n      26047.65\n      76276.200191\n      648941\n      175\n      4715\n    \n    \n      1\n      bottom\n      495.56\n      1220.756292\n      9124\n      3\n      141"
  },
  {
    "objectID": "02_analytics.html#sec-tags",
    "href": "02_analytics.html#sec-tags",
    "title": "3  Analiza danych",
    "section": "3.5 Analiza znaczników (tagów)",
    "text": "3.5 Analiza znaczników (tagów)\nW celu identyfikacji tematów zapytań na forum przeanalizowano słowa kluczowe, którymi oznaczano pytania. Wyniki przedstawiono w postaci wykresu typu “Chmura słów” Rys. 3.8. Okazuje się, że użytkownicy najczęściej pytali o wina (134, ang. wine), smak (120, ang. taste), historię (85, ang. history) i o rekomendacje (76, ang. recommandation).\nTę samą analizę wykonano na grupie 100 najlepiej i najgorzej ocenianych pytań badanych w Sekcja 3.4. W pierwszej grupie najczęściej używano znaczników smak (17) oraz warzenie (14, ang. brewing), w drugiej natomiast najczęściej pytano o wina (20), rekomendacje (15) i zdrowie (12, ang. health).\n\n\n\n\n\nRys. 3.8: Chmura słów przedstawiająca najczęściej używane znaczniki na forum\n\n\n\n\nNastępnie sprawdzono, które ze znaczników nie tylko były najczęściej używane, lecz które generowały najwięcej wyświetleń. W tym celu zsumowano liczbę wyświetleń wszystkich zapytań na forum, w którym pojawił się dany znacznik (Tab. 3.11). Znaczniki taste oraz health generują najwięcej wyświetleń na tym forum w ilości 1349051 oraz 1316064, na dalszych miejscach znajdują się znaczniki preservation (686106), storage (558949) oraz whiskey(470841).\n\n\n\n\n\n\nTab. 3.11:  Sumaryczne ilości wyświetleń wiadomości zawierających dany znacznik. Przedstawiono 10 najpopularniejszych znaczników. \n  \n    \n      \n      tag\n      liczba wyświetleń sumarycznie\n    \n  \n  \n    \n      0\n      taste\n      1330670\n    \n    \n      1\n      health\n      1286001\n    \n    \n      2\n      preservation\n      682216\n    \n    \n      3\n      storage\n      542860\n    \n    \n      4\n      whiskey\n      464756\n    \n    \n      5\n      bourbon\n      330268\n    \n    \n      6\n      brewing\n      307892\n    \n    \n      7\n      ipa\n      291935\n    \n    \n      8\n      spirits\n      255328\n    \n    \n      9\n      drinking\n      225924\n    \n  \n\n\n\n\n\nZbadano również czy wykorzystanie najczęściej używanych znaczników zmieniało się w czasie istnienia forum. Z danych zaprezentowanych na Rys. 3.9 można zaobserwować że niektóre znaczniki takie jak spirits, bourbon czy whiskey zaczęły być używane dopiero około roku 2016 - około 2 lata po pierwszej wiadomości na forum. W danych zauważono również, że częstość używania znaczników jest skokowa. Może to być związane z ogólną małą ilością postów zamieszczanych na forum. Wyjątek stanowią najpopularniejsze znaczniki, takie jak taste czy health, których częstość występowania jest bardziej regularna.\n\n\n\n\n\nRys. 3.9: Liczba postów w czasie dla każdego z najczęściej wyświetlanych znaczników. Wartości zagregowane na poziomie miesięcy. Górny panel przedstawia wartości miesięczne, dolny natomiast miesięczną sumę kumulatywną."
  },
  {
    "objectID": "02_analytics.html#sec-titles",
    "href": "02_analytics.html#sec-titles",
    "title": "3  Analiza danych",
    "section": "3.6 Analiza tytułów postów",
    "text": "3.6 Analiza tytułów postów\nDodatkową metodą, oprócz analizy znaczkików, którą można wykorzystać w celu zbadania tematów poruszanych na forum może być analiza najczęściej pojawiających się słów. Można do tego wykorzystać treść tytułów wiadomości jak i ich główną treść. W niniejszej pracy skupiono się na analizie treści tytułów wiadomości, jako że bardzo często zawierają one dużo słów kluczowych.\nTytuły wiadomości są nieustrukturyzowanymi ciągami znaków, z tego powodu do ich analizy wykorzystano powszechnie używane narzędzia stosowane przy analizach NLP (ang. Natural Language Processing). Proces zastosowanej tutaj analizy tekstu składał się z 4 kroków:\n\nWyodrębnienia tokenów\nUsunięcia tokenów o długości znaku 1\nUsunięcia tokenów nie niosących informacji o treści (ang. stop words)\nUjednolicenia różych form danego wyrazu poprzez zabieg stemming’u (ang. stemming), mającego na celu ucinanie końcówek wyrazów (tokenów) i pozostawieniu jedynie jego wartości bazowej.\n\nPrzykładowa tabela wizualizująca tokeny surowe w porównaniu do tokenów po odfiltrowaniu słów typu stop words oraz tokenów krótszych niż 1 zaprezentowano na Tab. 3.12\n\n\n\n\n\n\nTab. 3.12:  Przykładowe rekordy danych wizualizujące proces przetwarzania tekstu. Kolejność chronologiczna kolumn title –> words_token –> words_no_stop –> words_stem \n  \n    \n      \n      0\n    \n  \n  \n    \n      title\n      What is a citra hop, and how does it differ from other hops?\n    \n    \n      words_token\n      [what, is, citra, hop, and, how, does, it, differ, from, other, hops]\n    \n    \n      words_no_stop\n      [citra, hop, differ, hops]\n    \n    \n      words_stem\n      [citra, hop, differ, hop]\n    \n  \n\n\n\n\n\nDzięki zabiegowi stemmingu otrzymywany jest bardziej jednorodny zestaw danych. Liczba unikalnych tokenów została zredukowana z 2055 do 1680.\nNastępnie zliczono, które tokeny pojawiają się w największej ilości tytułów i zaprezentowano w Tab. 3.13. Najczęściej używanymi tokenami były tokeny beer (476), wine (152) oraz drink (105). Wyniki pozostają w zgodzie z wnioskami uzyskanymi poprzez analizę znaczników.\n\n\n\n\n\n\nTab. 3.13:  Zliczenia tokenów słów najczęściej pojawiających się w tytułach postów (15 najliczniejszych) \n  \n    \n      \n      words\n      count\n    \n  \n  \n    \n      0\n      beer\n      476\n    \n    \n      1\n      wine\n      147\n    \n    \n      2\n      drink\n      104\n    \n    \n      3\n      alcohol\n      88\n    \n    \n      4\n      differ\n      72\n    \n    \n      5\n      bottl\n      68\n    \n    \n      6\n      use\n      50\n    \n    \n      7\n      tast\n      47\n    \n    \n      8\n      brew\n      43\n    \n    \n      9\n      make\n      41\n    \n    \n      10\n      good\n      33\n    \n    \n      11\n      cocktail\n      29\n    \n    \n      12\n      age\n      27\n    \n    \n      13\n      ale\n      26\n    \n    \n      14\n      recommend\n      26"
  },
  {
    "objectID": "summary.html#budowa-infrastruktury",
    "href": "summary.html#budowa-infrastruktury",
    "title": "WNIOSKI I ZAKOŃCZENIE",
    "section": "Budowa infrastruktury",
    "text": "Budowa infrastruktury\nW niniejszej pracy przygotowano podstawowe środowisko pracy do analizy dużych zbiorów danych. Jako środowisko przechowywania danych wykorzystano serwis S3 który pozwala na przechowywanie wiele typów oraz formatów danych i jest łatwo skalowalny oraz posiada opcje optymalizacji oraz archiwizacji danych. Do zapisania danych w serwisie S3 wykorzystano usługę Cloud9, jednak równie dobrze użytkownik może wykonać opisane w tej pracy kroki na każdej maszynie (również poza chmurą AWS), wystarczy tylko wygenerowanie i użycie odpowiednich kluczy dostępu. Cloud9 wydaje się jednak prostszym rozwiązaniem, gdyż zawiera już zainstalowany program AWS CLI, który może być wykorzystany do zapisania danych w serwisie S3.\nCzęść analityczną zaprojektowano w sposób, aby jak najbardziej uprościć konfigurację by móc skupić się na przeprowadzeniu analizy danych. Wykorzystano popularne narzędzie Apache Spark (głównie moduł SQL z API pySpark) oraz serwis AWS EMR ze środowiskiem graficznym Jupyter Hub, bardzo popularnym wśród programistów/analityków korzystających z języka python. Dodatkowo, zadbano o to żeby efekty pracy (notebooki) oraz logi z klastra były automatycznie zapisywane w osobnych koszykach w serwisie S3.\nW przypadku potrzeby usprawnienia i optymailzacji rozwiązania, możliwa jest dodatkowa konfiguracja obecnej infrastruktury. Jedym z możliwych usprawnień jest reduckja zajmowanego miesjca w usłudze S3. Koszyk danych raw-data-beer-and-wine może posiadać automatyczne usuwanie danych lub archiwizację danych do usługi S3 Glacier, jako że dane w formacie xml po ekstrakcji i zapisaniu ich w formacie parquet są danymi nadmiarowymi.\nPotencjalny schemat tego rozwiązania mógłby składać się z następujących kroków:\n\nWykonywanie kodu z sekcji Sekcja 1.1, mogłoby zostać zautomatyzowane poprzez cykliczne uruchamianie skryptu o danej godzinie np. przy pomocy serwisu AWS Lambda bądź AWS Batch\nPo wykonaniu ekstrakcji, uruchamiany byłby kod do wstępnego przetwarzania danych zapisujący dane w formacie parquet opisany w Sekcja 1.2 oraz w szczegółach w Rozdział 2\nDane z koszyka raw-data-beer-and-wine byłyby usuwane np. godzinę po zakończeniu kroku numer 2.\n\nW przypadku prezentowanego w tej pracy przykładu optymalizacja przechowywania danych nie jest etapem kluczowym, ze względu na stosunkowo mały rozmiar danych, lecz będzie to miało istotne znaczenie w kosztach analizy, gdy rozmiar danych będzie zwiększony.\nKolejnym ulepszeniem w celu usprawnienienia tworzenias niezbędnej infrastruktury, jest zapisanie jej w postaci projektu typu Infrastructure as a Code. Pozwoliłoby to na śledzenie zmian w projekcie przy pomocy kontroli wersji, a także dodawanie kolejnych komponentów wspomnianych wyżej lub usuwanie komponentów w sposób programatyczny. Można w tym celu wykorzystać programy AWS CDK(„AWS CDK: Define your cloud application resources using familiar programming languages”, b.d.), Terraform(„Automate Infrastructure on Any Cloud”, b.d.) lub bezpośrednio AWS CloudFormation(„AWS CloudFormation: Speed up cloud provisioning with infrastructure as code”, b.d.)."
  },
  {
    "objectID": "summary.html#analiza-danych",
    "href": "summary.html#analiza-danych",
    "title": "WNIOSKI I ZAKOŃCZENIE",
    "section": "Analiza danych",
    "text": "Analiza danych\nW części analitycznej pracy zbadano charakterystykę jednego z forów internetowych. W Sekcja 3.1, Sekcja 3.2 oraz Sekcja 3.3 dokonano wstępnych analiz zachowań użytowników na forum, takich jak częstotliwość zadawania pytań, uzyskiwania odpowiedzi czy retencji na forum. Wyniki mogą posłużyć za wstęp do głębszych analiz przez właścicieli forum, w celu poszukiwania metod uatrakcyjnienia dostarczanego produktu, jako że forum z czasem znacząco traci na popularności.\nW kolejnych sekcjach (Sekcja 3.4, Sekcja 3.5, Sekcja 3.6) dokonano analizy treści oraz jakości zamieszczanych na forum informacji. Sekcja 3.4 zidentyfikowała najlepiej oraz najgorzej oceniane pytania. Ich dokłądniejsza analiza może posłużyć do zbadania treści tych wiadomości w celu stworzenia instrukcji dla nowych użytkowników o tym, co powinno znaleźć się w treści pytania, aby zwiększyć szanse na uzyskanie odpowiedzi. W Sekcja 3.5 oraz Sekcja 3.6 dokonano wstępnej analizy tematyki zadawanych pytań. Podejście opierało się na wyszukiwaniu najpopularniejszych słów kluczowych wśród znaczników (tagów) oraz tytułów postów. W celu pogłębienia tej analizy można spróbować wykorzystać techinkę tf-idf („TFIDF” 2020) w celu identyfikacji charakterystycznych słów, biorąc pod uwagę ich występowanie pomiędzy postami. Dodatkowo możliwe jest grupowanie podobnych do siebie postów przy pomocy algorytmu TextRank (Mihalcea i Tarau, b.d.). Algorytm ten można wykorzystać w celu oferowania podpowiedzi użytkownikom, którzy zadają pytanie na forum będące duplikatem już istniejącego.\n\n\n\n\n„Automate Infrastructure on Any Cloud”. b.d. HashiCorp Terraform. https://www.terraform.io/.\n\n\n„AWS CDK: Define your cloud application resources using familiar programming languages”. b.d. Amazon Web Services, Inc. https://aws.amazon.com/cdk/.\n\n\n„AWS CloudFormation: Speed up cloud provisioning with infrastructure as code”. b.d. Amazon Web Services, Inc. https://aws.amazon.com/cloudformation/.\n\n\nMihalcea, Rada, i Paul Tarau. b.d. „TextRank: Bringing Order into Texts”. https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf.\n\n\n„TFIDF”. 2020. Wikipedia. Wikimedia Foundation. https://pl.wikipedia.org/wiki/TFIDF."
  },
  {
    "objectID": "attachments.html#sec-emr",
    "href": "attachments.html#sec-emr",
    "title": "Appendix A — Budowanie infrastruktury chmurowej",
    "section": "A.1 Polecenie AWS CLI dla usługi EMR",
    "text": "A.1 Polecenie AWS CLI dla usługi EMR\naws emr create-cluster --name=\"MyEMRCluster\" \\\n    --release-label emr-6.8.0 \\\n    --applications Name=JupyterHub Name=Hadoop Name=Spark \\\n    --log-uri s3://emr-logs-beer-and-wine/MyJupyterClusterLogs \\\n    --use-default-roles \\\n    --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m4.xlarge, \\\n        BidPrice=OnDemandPrice  \n        InstanceGroupType=CORE,InstanceCount=2,InstanceType=m4.xlarge,BidPrice=OnDemandPrice \\\n    --ebs-root-volume-size 32 \\\n    --configurations file://emr-configurations.json \\\n    --bootstrap-actions \n        Path=s3://misc-beer-and-wine/install_python_libraries.sh,Name=InstallJupyterLibs"
  },
  {
    "objectID": "attachments.html#plik-install-my-jupyter-libraries.sh",
    "href": "attachments.html#plik-install-my-jupyter-libraries.sh",
    "title": "Appendix A — Budowanie infrastruktury chmurowej",
    "section": "A.2 Plik install-my-jupyter-libraries.sh",
    "text": "A.2 Plik install-my-jupyter-libraries.sh\nOdpowiada za zainstalowanie dodatkowych bibliotek pythonowych na węzłach typy master oraz worker. Wykorzystany w parametrze --bootstrap-actions Sekcja A.1 \n#!/bin/bash\n\nsudo python3 -m pip install matplotlib plotnine wordcloud bs4 html nltk pandas \\ \n    && patchworklib boto3 pyspark jupyter-client scikit-misc"
  },
  {
    "objectID": "attachments.html#sec-emr-config",
    "href": "attachments.html#sec-emr-config",
    "title": "Appendix A — Budowanie infrastruktury chmurowej",
    "section": "A.3 Plik emr-configurations.json",
    "text": "A.3 Plik emr-configurations.json\nOdpowiada za automatycznie zapisywanie notebooków w serwisie S3. Wykorzystany w parametrze --configurations Sekcja A.1. \n[\n    {\n        \"Classification\": \"jupyter-s3-conf\",\n        \"Properties\": {\n            \"s3.persistence.enabled\": \"true\",\n            \"s3.persistence.bucket\": \"emr-jupyter-notebooks-zjasdu123\"\n        }\n    }\n]"
  },
  {
    "objectID": "attachments.html#polecenie-aws-cli-dla-usługi-s3",
    "href": "attachments.html#polecenie-aws-cli-dla-usługi-s3",
    "title": "Appendix A — Budowanie infrastruktury chmurowej",
    "section": "A.4 Polecenie AWS CLI dla usługi S3",
    "text": "A.4 Polecenie AWS CLI dla usługi S3\nKoszyki S3 zostały utworzone przy pomocy poniższego polecenia:\naws s3api create-bucket --acl private --bucket <nazwa koszka>"
  },
  {
    "objectID": "functions.html#sec-tags-remove",
    "href": "functions.html#sec-tags-remove",
    "title": "Appendix B — Definicje funkcji",
    "section": "B.1 tags_remove()",
    "text": "B.1 tags_remove()\n\n\nKod\nfrom pyspark.sql.functions import udf\nfrom bs4 import BeautifulSoup\nfrom html import unescape\n\ndef tags_remove(s):\n    if s is not None:\n        soup = BeautifulSoup(unescape(s), 'lxml')\n        return soup.text\n    else: \n        return None\nudf_tags_remove = udf(lambda m: tags_remove(m))"
  },
  {
    "objectID": "functions.html#regexp_extract_all",
    "href": "functions.html#regexp_extract_all",
    "title": "Appendix B — Definicje funkcji",
    "section": "B.2 regexp_extract_all()",
    "text": "B.2 regexp_extract_all()\nŹródło: https://gist.github.com/dannymeijer/be3534470b205280e52dbbcbb19a9670 \n\n\nKod\nfrom pyspark.sql import DataFrame\nfrom pyspark.sql import functions as f\n\n\ndef regexp_extract_all(\n    df: DataFrame,\n    regex: str,\n    no_of_extracts: int,\n    input_column_name: str,\n    output_column_name: str = \"output\",\n    empty_array_replace: bool = True,\n):\n    \"\"\"Pyspark implementation for extracting all matches of a reg_exp_extract\n    \n    Background\n    ----------\n    The regular implementation of regexp_extract (as part of pyspark.sql.functions module)\n    is not capable of returning more than 1 match on a regexp string at a time. This \n    function can be used to circumvent this limitation.\n    \n    How it works\n    ------------\n    You can specify a `no_of_extracts` which will essentially run the regexp_extract \n    function that number of times on the `input_column` of the `df` (`DataFrame`). \n    In between extracts, a set of interim columns are created where every \n    intermediate match is stored. A distinct array is created from these matches, \n    after which the interim columns are dropped. The resulting array is stored in \n    the defined `output_column`. Empty strings/values in the resulting array can \n    optionally be dropped or kept depending on how `empty_array_replace` is set \n    (default is True).\n    \n    Usage example\n    -------------\n    In the below example, we are extracting all email-addresses from a body of text.\n    The returned DataFrame will have a new ArrayType column added named `email_addresses`\n    \n    > # Assuming `df` is a valid DataFrame containing a column named `text`\n    > email_regex = r\"[\\w.-]+@[\\w.-]+\\.[a-zA-Z]{1,}\"\n    > df = regexp_extract_all(df, email_regex, 6, \"text\", \"email_addresses\", True)\n    \n    Parameters\n    ----------\n    df: DataFrame\n        Input DataFrame\n    \n    regex: str\n        Regexp string to extract from input DataFrame\n    \n    no_of_extracts: int\n        Max number of occurrences to extract\n    \n    input_column_name: str\n        Name of the input column\n    \n    output_column_name: str\n        Name of the output column (default: output)\n    \n    empty_array_replace: bool\n        If set to True, will replace empty arrays with null values (default: True)\n    \"\"\"\n    repeats = range(0, no_of_extracts)\n\n    # A set of interim columns are created that will be dropped afterwards\n    match_columns = [f\"___{r}___\" for r in repeats]\n\n    # Apply regexp_extract an r number of times\n    for r in repeats:\n        df = df.withColumn(\n            match_columns[r],\n            f.regexp_extract(\n                f.col(input_column_name),\n                # the input regex string is amended with \".*?\"\n                # and repeated an r number of times\n                # r needs to be +1 as matching groups are 1-indexed\n                \"\".join([f\"{regex}.*?\" for i in range(0, r + 1)]),\n                r + 1,\n            ),\n        )\n\n    # Create a distinct array with all empty strings removed\n    df = df.withColumn(\n        output_column_name,\n        f.array_remove(f.array_distinct(f.array(match_columns)), \"\"),\n    )\n\n    # Replace empty string with None if empty_array_replace was set\n    if empty_array_replace:\n        df = df.withColumn(\n            output_column_name,\n            f.when(f.size(output_column_name) == 0, f.lit(None)).otherwise(\n                f.col(output_column_name)\n            ),\n        )\n\n    # Drop interim columns\n    for c in match_columns:\n        df = df.drop(c)\n\n    return df"
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Appendix C — Repozytorium kodu wykorzystanego w pracy",
    "section": "",
    "text": "https://github.com/michkam89/big-data-pw-project"
  }
]